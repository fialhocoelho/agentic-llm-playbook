# Week 3: Post-Training Alignment

Coming soon...

## Planned Topics

- Supervised Fine-Tuning (SFT)
- Reward modeling
- Direct Preference Optimization (DPO)
- Reinforcement Learning from Human Feedback (RLHF)
- Group Relative Policy Optimization (GRPO)
- Evaluation of aligned models
