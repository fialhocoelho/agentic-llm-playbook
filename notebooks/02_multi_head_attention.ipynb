{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Transformers\n",
    "\n",
    "## Notebook 02: Multi-Head Attention and Transformer Blocks\n",
    "\n",
    "This notebook explores Multi-Head Attention (MHA) and how it enables the model to attend to different representation subspaces.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the motivation for multi-head attention\n",
    "- Implement and test multi-head attention\n",
    "- Build a complete transformer block\n",
    "- Analyze the role of feed-forward networks and residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from src.llm_journey.models import MultiHeadAttention, TransformerBlock\n",
    "from src.llm_journey.utils import set_seed, count_parameters\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, multi-head attention projects the queries, keys, and values $h$ times with different learned linear projections.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is:\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n",
    "\n",
    "This allows the model to jointly attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention module\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads, dropout=0.1)\n",
    "print(f\"Multi-Head Attention parameters: {count_parameters(mha)}\")\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = mha(x, x, x)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Multiple Heads?\n",
    "\n",
    "Multiple heads allow the model to:\n",
    "- Attend to different positions simultaneously\n",
    "- Capture different aspects of the relationships between tokens\n",
    "- Learn different attention patterns for syntax, semantics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single-head vs multi-head\n",
    "configs = [\n",
    "    {\"num_heads\": 1, \"d_model\": 64},\n",
    "    {\"num_heads\": 4, \"d_model\": 64},\n",
    "    {\"num_heads\": 8, \"d_model\": 64},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    mha = MultiHeadAttention(config[\"d_model\"], config[\"num_heads\"])\n",
    "    params = count_parameters(mha)\n",
    "    print(f\"Heads: {config['num_heads']:2d} | Parameters: {params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Block\n",
    "\n",
    "A transformer block combines:\n",
    "1. Multi-head self-attention\n",
    "2. Feed-forward network (two linear layers with activation)\n",
    "3. Layer normalization\n",
    "4. Residual connections\n",
    "\n",
    "$$\\text{TransformerBlock}(x) = \\text{LayerNorm}(x + \\text{FFN}(\\text{LayerNorm}(x + \\text{MHA}(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer block\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256  # Feed-forward dimension (typically 4x d_model)\n",
    "\n",
    "block = TransformerBlock(d_model, num_heads, d_ff, dropout=0.1)\n",
    "print(f\"Transformer block parameters: {count_parameters(block):,}\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Residual Connections and Layer Normalization\n",
    "\n",
    "These components are crucial for training deep networks:\n",
    "- **Residual connections**: Enable gradient flow through deep networks\n",
    "- **Layer normalization**: Stabilizes training and speeds up convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate residual connections\n",
    "x = torch.randn(1, 5, d_model)\n",
    "output = block(x)\n",
    "\n",
    "# The output should be different from input but preserves information flow\n",
    "print(f\"Input mean: {x.mean():.4f}, std: {x.std():.4f}\")\n",
    "print(f\"Output mean: {output.mean():.4f}, std: {output.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking Transformer Blocks\n",
    "\n",
    "Modern LLMs stack many transformer blocks (12, 24, or even 96 layers). Let's see how this scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack multiple transformer blocks\n",
    "num_layers_list = [1, 6, 12, 24]\n",
    "\n",
    "for num_layers in num_layers_list:\n",
    "    blocks = nn.ModuleList([TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "    total_params = sum(count_parameters(block) for block in blocks)\n",
    "    print(f\"Layers: {num_layers:2d} | Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Experiment with different numbers of attention heads and observe parameter counts\n",
    "2. Modify the feed-forward dimension ratio (currently 4x) and analyze the impact\n",
    "3. Implement a function to visualize attention weights for each head\n",
    "4. Compare the output distributions with and without layer normalization\n",
    "5. Stack multiple transformer blocks and pass data through the entire stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue to Notebook 03 to build a complete language model with positional encoding and learn about training procedures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Playbook",
   "language": "python",
   "name": "llm-playbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
