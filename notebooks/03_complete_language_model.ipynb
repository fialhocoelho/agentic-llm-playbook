{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Transformers\n",
    "\n",
    "## Notebook 03: Building a Complete Language Model\n",
    "\n",
    "This notebook ties everything together to build a complete character-level language model.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand positional encoding\n",
    "- Build a complete transformer-based language model\n",
    "- Prepare data for training\n",
    "- Understand the training loop structure\n",
    "- Generate text with a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from llm_journey.data import SimpleTokenizer, TextDataset\n",
    "from llm_journey.models import TransformerBlock\n",
    "from llm_journey.utils import set_seed, count_parameters\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Tokenizing Data\n",
    "\n",
    "We'll use our tiny corpus for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "with open('../data/tiny_corpus.txt', 'r') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(f\"Corpus length: {len(corpus)} characters\")\n",
    "print(f\"First 200 characters:\\n{corpus[:200]}...\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = SimpleTokenizer(corpus)\n",
    "print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Vocabulary: {sorted(tokenizer.char_to_idx.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokens = tokenizer.encode(corpus)\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"First 50 tokens: {tokens[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "seq_length = 32\n",
    "dataset = TextDataset(tokens, seq_length)\n",
    "print(f\"Dataset size: {len(dataset)} sequences\")\n",
    "\n",
    "# Create dataloader\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Sample a batch\n",
    "x_batch, y_batch = next(iter(dataloader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Input: {x_batch.shape}\")\n",
    "print(f\"  Target: {y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding\n",
    "\n",
    "Transformers have no inherent notion of position. We add positional information using sinusoidal functions:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "Alternatively, we can use learned positional embeddings (which we'll use here for simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"A simple transformer-based language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Token embeddings + positional embeddings\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Final layer norm and projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SimpleLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_ff=256,\n",
    "    max_seq_len=128,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "logits = model(x_batch)\n",
    "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
    "print(f\"Expected shape: (batch_size={batch_size}, seq_len={seq_length}, vocab_size={tokenizer.vocab_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop Structure (Outline)\n",
    "\n",
    "Here's the structure for training the model. Full training would take longer and require more data.\n",
    "\n",
    "```python\n",
    "# Training pseudocode\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # Forward pass\n",
    "        logits = model(x_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation (Conceptual)\n",
    "\n",
    "Once trained, we generate text by:\n",
    "1. Starting with a prompt (seed text)\n",
    "2. Encoding the prompt\n",
    "3. Feeding through the model to get logits\n",
    "4. Sampling the next token from the distribution\n",
    "5. Appending the token and repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text from a prompt (requires trained model).\"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Prepare input\n",
    "            x = torch.tensor(tokens).unsqueeze(0)\n",
    "            \n",
    "            # Get logits\n",
    "            logits = model(x)\n",
    "            logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "# This would work with a trained model:\n",
    "# generated_text = generate(model, tokenizer, \"The quick\", max_length=100)\n",
    "# print(generated_text)\n",
    "\n",
    "print(\"Generation function defined (requires trained model to use).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "- Data loading and tokenization\n",
    "- Creating datasets and dataloaders\n",
    "- Positional encoding\n",
    "- Building a complete language model\n",
    "- Training loop structure\n",
    "- Text generation procedure\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement sinusoidal positional encoding and compare with learned embeddings\n",
    "2. Add temperature scaling to control generation randomness\n",
    "3. Implement top-k and nucleus (top-p) sampling\n",
    "4. Train the model for a few epochs and observe the loss curve\n",
    "5. Generate samples at different temperatures and compare the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Week 2 will cover training fundamentals, optimization techniques, and scaling considerations. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Playbook",
   "language": "python",
   "name": "llm-playbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
